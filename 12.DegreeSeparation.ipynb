{"cells":[{"cell_type":"markdown","metadata":{"id":"1krgsG2yb98a"},"source":["# Tutorial: Taming Big Data With Apache Spark and Python - Hands On!\n","## Exercise 7 - Super Hero Degrees of Separation\n","\n","### Setup\n","\n","FindSpark\n","\n","This will circumvent many issues with your system finding spark"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cy8wGloYV3Wt","executionInfo":{"status":"ok","timestamp":1663215069761,"user_tz":-420,"elapsed":23011,"user":{"displayName":"Bagas Pambudi","userId":"16684062612564440299"}},"outputId":"b4f6493c-eb74-4624-df6f-8cdf5d0a9ed9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!wget https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n","!tar -xvf spark-2.4.5-bin-hadoop2.7.tgz\n","!mv spark-2.4.5-bin-hadoop2.7 spark-2.4.5"],"metadata":{"id":"LvAqitsoWYpb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","# Install java\n","!apt-get update -qq\n","!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null \n","\n","!pip install -q findspark\n"," \n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5\"\n","!java -version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l8-EtFgSVWW-","executionInfo":{"status":"ok","timestamp":1663215307806,"user_tz":-420,"elapsed":11383,"user":{"displayName":"Bagas Pambudi","userId":"16684062612564440299"}},"outputId":"ba919ccf-0cf3-4825-a988-244fbac53e75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["openjdk version \"1.8.0_342\"\n","OpenJDK Runtime Environment (build 1.8.0_342-8u342-b07-0ubuntu1~18.04-b07)\n","OpenJDK 64-Bit Server VM (build 25.342-b07, mixed mode)\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/bangkit-pambudi/resource-spark.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7vbC0-dXRAe","executionInfo":{"status":"ok","timestamp":1663215415080,"user_tz":-420,"elapsed":3894,"user":{"displayName":"Bagas Pambudi","userId":"16684062612564440299"}},"outputId":"406b29f4-2de3-46f3-b83c-270bfa104066"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'resource-spark'...\n","remote: Enumerating objects: 38, done.\u001b[K\n","remote: Counting objects: 100% (38/38), done.\u001b[K\n","remote: Compressing objects: 100% (36/36), done.\u001b[K\n","remote: Total 38 (delta 7), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (38/38), done.\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_boxr8aPb98h"},"outputs":[],"source":["import findspark\n","findspark.init()"]},{"cell_type":"markdown","metadata":{"id":"HHZVCQwDb98n"},"source":["Load Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8h_ZpNsb98p"},"outputs":[],"source":["from pyspark import SparkConf, SparkContext"]},{"cell_type":"markdown","metadata":{"id":"TM_eAGh8b98s"},"source":["Set the file path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGf5Fn15b98t"},"outputs":[],"source":["data_folder = \"/content/resource-spark/data/\""]},{"cell_type":"markdown","metadata":{"id":"4HxqLgIab98v"},"source":["Create the Spark Context"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1P151dFxb98y"},"outputs":[],"source":["# configure your Spark context; master node is local machine\n","conf = SparkConf().setMaster(\"local\").setAppName(\"DegreesOfSeparation\")\n","\n","# create a spark context object\n","sc = SparkContext(conf = conf)"]},{"cell_type":"markdown","metadata":{"id":"ufNVHThJb980"},"source":["Establish search parameters: startCharacterID and targetCharacterID."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byBjI-1Gb982"},"outputs":[],"source":["startCharacterID = 5306 #SpiderMan\n","targetCharacterID = 14 #ADAM 3,031 (who?)"]},{"cell_type":"markdown","metadata":{"id":"S4A4jGZmb984"},"source":["Define and set the accumulator to 0."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YA9JNqDXb985"},"outputs":[],"source":["hitCounter = sc.accumulator(0)"]},{"cell_type":"markdown","metadata":{"id":"PSUvrWGUb988"},"source":["### Define Functions\n","\n","Convert each line of our input Rdd to a format for BFS. It will be a key-value pair, with the key being the heroID and the value being the connections, the distance and the colour. We also initialize our startCharacterID."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0z1rvTfb988"},"outputs":[],"source":["def convertToBFS(line):\n","    fields = line.split()\n","    heroID = int(fields[0])\n","    connections = []\n","    for connection in fields[1:]:\n","        connections.append(int(connection))\n","    \n","    colour = 'WHITE' #unprocessed\n","    distance = 9999 #infinite distance\n","    \n","    if (heroID == startCharacterID):\n","        colour = 'GRAY'\n","        distance = 0\n","    \n","    return (heroID, (connections, distance, colour))"]},{"cell_type":"markdown","metadata":{"id":"ANPNOL_cb98-"},"source":["Bring in your data and convert to BFS."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVmNzAUkb98_"},"outputs":[],"source":["def createStartingRdd():\n","    file_to_open = data_folder + \"marvel-graph.txt\"\n","    inputFile = sc.textFile(file_to_open)\n","    return inputFile.map(convertToBFS)"]},{"cell_type":"markdown","metadata":{"id":"4TZa1undb99A"},"source":["Define the mapper."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eS1Qclmbb99B"},"outputs":[],"source":["def bfsMap(node):\n","    characterID = node[0] #heroID\n","    data = node[1] #connections, distance, colour\n","    connections = data[0]\n","    distance = data[1]\n","    colour = data[2]\n","    \n","    results = []\n","    \n","    #if this node needs to be expanded...\n","    if (colour == 'GRAY'):\n","        for connection in connections:\n","            newCharacterID = connection\n","            newDistance = distance + 1\n","            newColour = 'GRAY'\n","            if (targetCharacterID == connection):\n","                hitCounter.add(1)\n","            \n","            newEntry = (newCharacterID, ([], newDistance, newColour))\n","            print(type(newEntry))\n","            results.append(newEntry)\n","        \n","        #Since we've processed this node, colour it black\n","        colour = 'BLACK'\n","        \n","    #Emit the input node so we don't lose it\n","    print(type(results))\n","    results.append((characterID, (connections, distance, colour)))\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"ztLYusXnb99D"},"source":["Define the reducer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HdmI3GBBb99E"},"outputs":[],"source":["def bfsReduce(data1, data2):\n","    edges1 = data1[0]\n","    edges2 = data2[0]\n","    distance1 = data1[1]\n","    distance2 = data2[1]\n","    colour1 = data1[2]\n","    colour2 = data2[2]\n","    \n","    distance = 9999\n","    colour = colour1\n","    edges = []\n","    \n","    #See if one is the orignal node with its connections.\n","    # If so preserve them.\n","    # .extend() adds multiple elements to end of a list\n","    if(len(edges1) > 0):\n","        edges.extend(edges1)\n","    if(len(edges2) > 0):\n","        edges.extend(edges2)\n","        \n","    # Preserve minimum distance\n","    if(distance1 < distance):\n","        distance = distance1\n","        \n","    if(distance2 < distance):\n","        distance = distance2\n","        \n","    # Preserve darkest colour\n","    if(colour1 == 'WHITE' and (colour2 == 'GRAY' or colour2 == 'BLACK')):\n","        colour = colour2\n","        \n","    if(colour1 == 'GRAY' and colour2 == 'BLACK'):\n","        colour = colour2\n","        \n","    if(colour2 == 'WHITE' and (colour1 == 'GRAY' or colour1 == 'BLACK')):\n","        colour = colour1\n","        \n","    if(colour2 == 'GRAY' and colour1 == 'BLACK'):\n","        colour = colour1\n","    \n","    return(edges, distance, colour)"]},{"cell_type":"markdown","metadata":{"id":"xRV08WoRb99G"},"source":["### Main Program"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJlXALCDb99H"},"outputs":[],"source":["iterationRdd = createStartingRdd()"]},{"cell_type":"markdown","metadata":{"id":"vyUFZAUXb99I"},"source":["We are assuming that degrees of separation will not exceed 10. We use flatMap(bfsMap) to find GREY nodes, expand them and then colour them BLACK. Map functions are transforms, due to lazy processing they do not cause anything to happen. So we call an action (i.e., count) to force it to get evaluated."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ywp_yu5ob99J","outputId":"1caa6006-e6bb-4563-cffa-d8b3afca867a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running BFS iteration# 1\n","Processing 8330 values.\n","Running BFS iteration# 2\n","Processing 220615 values.\n","Hit the target character! From 1 different direction(s).\n"]}],"source":["for iteration in range(0, 10):\n","    print(\"Running BFS iteration# \" + str(iteration+1))\n","    \n","    # Create new vertices as needed to darken or reduce distances in the \n","    # reduce stage. If we encounter the node we're looking for as a GRAY\n","    # node, increment our accumulator to signal that we're done.\n","    mapped = iterationRdd.flatMap(bfsMap)\n","    \n","    # Note that mapped.count() action here forces the RDD to be evaluated, and\n","    # that's the only reason our accumulator is actually updated.\n","    print(\"Processing \" + str(mapped.count()) + \" values.\")\n","    \n","    if (hitCounter.value > 0):\n","        print(\"Hit the target character! From \" + str(hitCounter.value) \\\n","              + \" different direction(s).\")\n","        break\n","        \n","    # Reducer combines data for each character ID, preserving the darkest\n","    # colour and shortest path.\n","    iterationRdd = mapped.reduceByKey(bfsReduce)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}